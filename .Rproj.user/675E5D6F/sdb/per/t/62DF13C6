{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Visualizing Word2vec word vectors using the superheat R package\"\nauthor: \"Rebecca Barter\"\ndate: \"Last revision: September 27, 2016\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n\n\n\n## An speedy introduction to word2vec\n\nTo blatantly quote the [Wikipedia article](https://en.wikipedia.org/wiki/Word2vec) on Word2vec:\n\n> Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a high-dimensional space (typically of several hundred dimensions), with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space\n\n\nBasically, Word2vec was develpoed by a bunch of Googlers as a way to represent words as high-dimensional vectors in such a way that the relative positions of these vectors in space is meaningful in terms of linguistic context. Somehow, this embedding gives rise to algorithmic magic in which equations such as \n\n<p style=\"text-align: center;\"markdown=\"1\"> **king - queen = man - woman**</p>\n\nand its logical extension,\n\n<p style=\"text-align: center;\"markdown=\"1\"> **king - queen + woman = man** </p>\n\nmake sense.\n\n\nMost techniques for visualizing relationships between high-dimensional word vectors rely on dimensionality reduction techniques, such as t-SNE, some of which are [pretty cool](http://benschmidt.org/word2vec_map/). \n\nI'm going introduce an alternative approach to visualizing the relationship between word vectors in R that does not rely on dimensionality reduction. Welcome to the beautiful world of superheatmaps!\n\n\n\n## Preparing the data\n\n```{r, message=FALSE, warning=FALSE}\n# load in some useful libraries\nlibrary(knitr)\nlibrary(tibble)\nlibrary(dplyr)\n```\n\n\nThe Google News word2vec data can be found at the following github repository: https://github.com/mmihaltz/word2vec-GoogleNews-vectors. The best way to download the data is to simply clone the repository.\n\n\n\n### Identifying the most common words from the NY Times headlines\n\nFirst, we want to limit our data to some common words. Specifically, we will identify the most common words from a separate NTYimes dataset from the `maxent` R package.\n```{r, message=FALSE, warning=FALSE}\nlibrary(RTextTools)\n# The NYTimes dataset can be extracted from the RTextTools package\ndata(NYTimes)\n# view the first 6 rows\nkable(head(NYTimes, 3))\n```\n\nLet's first restrict to the headlines. Our goal is to obtain word counts.\n\n```{r, warning=FALSE, message=FALSE}\nlibrary(tm)\n# Extract the title from each article entry\nnyt.headlines <- as.character(NYTimes$Title)\n# convert the headline titles to a tm corpus\nnyt.corpus <- Corpus(VectorSource(nyt.headlines))\n```\n\n\nWe first need to remove stopwords like \"a\", \"and\", \"the\", etc, convert all words to lowercase, remove punctuation, numbers and spacing.\n```{r}\n# convert to lowercase\nnyt.corpus <- tm_map(nyt.corpus, content_transformer(tolower))\n# remove punctuation\nnyt.corpus <- tm_map(nyt.corpus, removePunctuation)\n# remove numebrs\nnyt.corpus <- tm_map(nyt.corpus, removeNumbers)\n# remove whitespace\nnyt.corpus <- tm_map(nyt.corpus, stripWhitespace)\n# remove stop words\nnyt.corpus <- tm_map(nyt.corpus, removeWords, stopwords(\"english\"))\n\n```\n\nNext to get the word counts, we can add up the columns of the document term matrix.\n```{r}\n# convert to a dtm\ndtm <- DocumentTermMatrix(nyt.corpus)\n# calcualte the word counts\nfreq <- colSums(as.matrix(dtm))   \n# view the 10 most frequent words\nkable(head(sort(freq, decreasing = T), 10))\n```\n\nApparently the articles in the dataset talk a lot about Bush, the Iraq war and the presidential campaigns!\n\nFiltering to the 1,000 most common words:\n\n```{r}\ncommon.words <- names(sort(freq, decreasing = TRUE))[1:1000]\n```\n\n\n### Obtaining the word2vec Google News data \n\n\nNext, we will load in the previously processed GoogleNews word2vec data, and filter to the most common words.\n\n```{r}\nload(\"data/GoogleNews.RData\")\nGoogleNews.common <- GoogleNews[tolower(GoogleNews[,1]) %in% common.words,]\n\n# the first column contains the word\n# convert the row naems to the word\nrownames(GoogleNews.common) <- GoogleNews.common[,1]\n# remove the first column\nGoogleNews.common <- GoogleNews.common[,-1]\n```\n\n\n\nStrangely, even though we only included the 1,000 most common words, the data consists of 2,674 word vectors of length 300.\n```{r}\n# the dimension of the data\ndim(GoogleNews.common)\n```\n\nIt turns out that there are many repetitions with different captializations. \n\n\n\n```{r}\n# view a small subset of the data\nkable(GoogleNews.common[1:5, 1:5])\n```\n\n\nIf we count how many unique lowercase words there are, we obtain a number closer to 1,000.\n\n```{r}\n# the number of unique words (ignoring case)\nlength(unique(tolower(rownames(GoogleNews.common))))\n```\n\n\nTo make the following analysis easier, let's make a data frame consisting of each word and it's lowercase version\n\n```{r}\nGoogle.words <- tibble(word = as.character(rownames(GoogleNews.common)),\n                       lowercase.word = as.character(tolower(rownames(GoogleNews.common))))\n\n```\n\n\n\nThe 9 words from the list of 1,000 most common words from the NY Times headlines that are missing from the Google News corpus are the following:\n\n```{r}\n# list the common words that do not appear in the Google News Corpus\ncommon.words[!(common.words %in% unique(Google.words$lowercase.word))]\n```\n\n\n### Comparing versions of the same word\n\nThe next natural question is whether the word vectors for the capilized words are similar in some sense to the word vectors for their lowercase versions.\n\nLet's look and see which words have the most variants:\n\n```{r}\nGoogle.words %>% \n  group_by(lowercase.word) %>% # group by the lowercase word version\n  mutate(n.versions = n()) %>% # count the number of entries for each lowercase word\n  arrange(desc(n.versions), lowercase.word) %>% # arrange data frame first by number of versions, then by word\n  head(13) %>% # look at the first 13 entries\n  kable # present as a kable for nice markdown incorporation!\n```\n\n## Introducing Superheat\n```{r, message=FALSE, warning=FALSE}\nlibrary(superheat)\n```\n\n\n\n\n### Visualizing raw word vectors\n\nLet's compare the word vectors for the various versions of the word \"next\".\n\n\n```{r}\n# restrict to the vectors for the word \"next\"\nnextword.vectors <- GoogleNews.common[tolower(rownames(GoogleNews.common)) == \"next\", ]\n```\n\n```{r, fig.align=\"center\"}\n# visualize the word vectors for each version of next using superheat\nsuperheat(nextword.vectors, heat.col.scheme = \"viridis\", title = \"Word vectors for each version of 'next'\")\n```\n\n\n### Visualizing cosine similarity\n\nIt is fairly obvious that the vectors are not the same, which admittedly isn't all that surprising, but should induce caution. Perhaps the vectors are at least correlated with one another? It is common to use cosine distance to compare word vectors.\n\n```{r}\nCosineFun <- function(x,y){\n  # calculate the cosine similarity between x and y\n  c <- sum(x*y) / (sqrt(sum(x * x)) * sqrt(sum(y * y)))\n  return(c)\n}\n\nCosineSim <- function(x) {\n  # initialize similarity matrix\n  m <- matrix(NA, \n              nrow = ncol(x),\n              ncol = ncol(x),\n              dimnames = list(colnames(x), colnames(x)))\n  cos <- as.data.frame(m)\n  \n  # calculate the pairwise cosine similarity\n  for(i in 1:ncol(x)) {\n    for(j in i:ncol(x)) {\n      co_rate_1 <- x[which(x[, i] & x[, j]), i]\n      co_rate_2 <- x[which(x[, i] & x[, j]), j]  \n      cos[i, j] <- CosineFun(co_rate_1, co_rate_2)\n      # fill in the opposite diagonal entry\n      cos[j, i] <- cos[i, j]        \n    }\n  }\n  return(cos)\n}\n\n\n```\n\n\nPlotting the cosine similarity between each version of the word \"next\", it is fairly clear that the vector for \"next\" is most similar to the vector for \"Next\", while the other word vectors are less similar to one another.\n\n\n\n```{r, fig.align='center'}\nnextword.cosine.similarity <- CosineSim(t(nextword.vectors))\n# make the diagnoal NA\ndiag(nextword.cosine.similarity) <- NA\n# plot a superheatmap\nsuperheat(nextword.cosine.similarity, title = \"Cosine similarity matrix for the 'next' word vectors\")\n```\n\n\nMoving forward we make the decision to ignore all strange capitalizations and use the lowercase version of each word.\n\n\n```{r}\n# identify all lowercase words\nlowercase.words <- rownames(GoogleNews.common) == tolower(rownames(GoogleNews.common))\n# restrict to these lowercase words\nGoogleNews.common <- GoogleNews.common[lowercase.words, ]\n# identify the dimension of the new dataset\ndim(GoogleNews.common)\n```\n\n\n```{r}\n# identify the 30 most common words that are also in the Google News corpus\nthirty.most.commmon.words <- common.words[1:30][common.words[1:30] %in% rownames(GoogleNews.common)]\n# calcualte the cosine similarity between the thirty most common words\ncosine.similarity <- CosineSim(t(GoogleNews.common[thirty.most.commmon.words, ]))\n```\n\n```{r}\ndiag(cosine.similarity) <- NA\n```\n\n```{r, fig.align='center', fig.width = 10, fig.height = 10}\nsuperheat(cosine.similarity, \n          \n          # set color scheme to viridis\n          heat.col.scheme = \"viridis\",\n          \n          # place dendrograms on columns and rows \n          yt.plot.type = \"dendrogram\", \n          yr.plot.type = \"dendrogram\",\n          \n          # make gridlines white for enhanced prettiness\n          grid.hline.col = \"white\",\n          grid.vline.col = \"white\",\n          \n          # rotate bottom label text\n          bottom.label.text.angle = 90)\n```\n\n\n### Visualizing word cluster similarity\n",
    "created" : 1474997896588.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1125835848",
    "id" : "62DF13C6",
    "lastKnownWriteTime" : 1475014702,
    "last_content_update" : 1475014702087,
    "path" : "~/Google Drive/Berkeley PhD/Packages/Supervised Heatmaps/tutorials/word2vec_superheat_example.Rmd",
    "project_path" : null,
    "properties" : {
        "last_setup_crc32" : "9D6E2462bb338d19",
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}
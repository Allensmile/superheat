{
    "collab_server" : "",
    "contents" : "\nlibrary(superheat)\nlibrary(cluster)\nlibrary(RColorBrewer)\nlibrary(tm)\nlibrary(dplyr)\n\n# load the cosine dissimilarity matrix\nload(\"/Users/rebeccabarter/Google Drive/Berkeley PhD/Research/Datasets/GoogleNews/GoogleNews_uppercase_cos.RData\")\n\n# try from 10 to 50 clusters\nset.seed(1234)\nif (!file.exists(\"GoogleNews_clust_10to50.RData\")) {\n  clust <- list()\n  for(i in 10:50) { print(i)\n    clust[[i]] <- pam(GoogleNews_uppercase_cos, k = i, diss = T)\n  }\n  clust_10to50 <- clust[-c(1:9)]\n  save(clust_10to50, file = \"GoogleNews_clust_10to5.RData\")\n} else {\n  load(\"/Users/rebeccabarter/Google Drive/Berkeley PhD/Research/Datasets/GoogleNews/GoogleNews_clust_10to50_top2000.RData\")\n}\n\n# calcualte silhouette for each cluster number\nsil <- lapply(clust_10to50, silhouette)\navg_sil <- sapply(sil, function(x) median(summary(x)[[2]]))\nplot(x = 10:50, y = avg_sil)\nbest_k <- (10:50)[which.max(avg_sil)]\nbest_k <- 10\n\n\n\nclust_obj <- pam(GoogleNews_uppercase_cos, k = best_k, diss = T)\nmedoid <- clust_obj$medoids # calcualte the medoid of each cluster\nclust <- factor(clust_obj$clustering)\nlevels(clust) <- medoid\n# name each cluster by the most frequent word\n# levels(clust) <- names(sapply(unique(clust), function(cl) {\n#   which.max(table(words)[tolower(names(clust[clust == cl]))])\n#   }))\n\n# calcualte silhouette width for each cluster\nsil_widths <- summary(sil[[which(10:50 == best_k)]])[[2]]\nnames(sil_widths) <- levels(clust)\n# rearrange clusters in order of silhouette widths\norder <- order(sil_widths)\n\ndata.frame(med = medoid, freq = levels(clust))\n\n#clust <- factor(clust, levels(clust)[order])\n\n\n\n# word frequencies\nfrequencies <- table(words)[tolower(names(clust))]\n\n# silhouette widths\ncounts <- table(clust)\nwidths <- rep(sil_widths[1], counts[1])\nfor(i in 2:length(counts)) {\n widths <- c(widths,rep(sil_widths[i], counts[i]))\n}\n\n# select 10 clusters with the largest widths\nset.seed(1236541)\nX <- 1 - GoogleNews_uppercase_cos\n\n# rank based on quality of cluster and size of cluster (we want the clusters which are large AND have large widths)\nranking <- order(order(unique(widths), decreasing = F)) #+ order(order(counts, decreasing = T))\n\nselected_clusters <- as.character(unique(clust)[which(ranking %in% sort(ranking, decreasing = F)[1:5])])\n\n# randomly select 10 word clusters\n#selected_clusters <- names(sort(counts, decreasing = T))[1:10]\nclust_sample <- clust[clust %in% selected_clusters]\nX_sample <- X[names(clust_sample), names(clust_sample)]\nfrequencies_sample <- frequencies[tolower(names(clust_sample))]\nwidths_sample <- widths[names(widths) %in% selected_clusters]\n  \n\n\n\nsuperheat(X = X_sample, # plot SIMILARITY matrix\n          membership.rows = clust_sample,\n          membership.cols = clust_sample,\n          heat.pal = c(\"red4\",brewer.pal(7, \"RdBu\"), \"midnightblue\"),\n          heat.pal.values = c(0, 0.3, 0.4, 0.47, 0.5, 0.53, 0.6, 0.8, 1),\n          bottom.text.angle = 90,\n          bottom.label.size = 0.2,\n          left.text.angle = 0,\n          left.label.size = 0.2,\n          box.size = 1.5,\n          \n          yt = frequencies_sample,\n          yt.plot.type = \"boxplot\",\n          \n          yr = widths_sample,\n          yr.plot.type = \"line\")\n          #bottom.heat.label = \"none\",\n          #left.heat.label = \"none\")\n\n\n\n################# LOWERCASE ######################\n\n\n# load the cosine dissimilarity matrix\nload(\"/Users/rebeccabarter/Google Drive/Berkeley PhD/Research/Datasets/GoogleNews/GoogleNews_lowercase_cos.RData\")\n\n# try from 10 to 50 clusters\nset.seed(1234)\nif (!file.exists(\"GoogleNews_clust_10to50_lowercase_top2000.RData\")) {\n  clust <- list()\n  for(i in 10:50) { print(i)\n    clust[[i]] <- pam(GoogleNews_lowercase_cos, k = i, diss = T)\n  }\n  clust_10to50 <- clust[-c(1:9)]\n  save(clust_10to50, file = \"/Users/rebeccabarter/Google Drive/Berkeley PhD/Research/Datasets/GoogleNews/GoogleNews_clust_10to5_lowercase_top2000.RData\")\n} else {\n  load(\"/Users/rebeccabarter/Google Drive/Berkeley PhD/Research/Datasets/GoogleNews/GoogleNews_clust_10to5_lowercase_top2000.RData\")\n}\n\n# calcualte silhouette for each cluster number\nsil <- lapply(clust_10to50, silhouette)\navg_sil <- sapply(sil, function(x) median(summary(x)[[2]]))\nplot(x = 10:50, y = avg_sil)\nbest_k <- (10:50)[which.max(avg_sil)]\nbest_k <- 44\n\n\n\n\nclust_obj <- pam(GoogleNews_lowercase_cos, k = best_k, diss = T)\nmedoid <- clust_obj$medoids # calcualte the medoid of each cluster\nclust <- factor(clust_obj$clustering)\nlevels(clust) <- medoid\n# name each cluster by the most frequent word\n#levels(clust) <- names(sapply(unique(clust), function(cl) {\n#  which.max(table(words)[tolower(names(clust[clust == cl]))])\n#}))\n\n\n# calcualte silhouette width for each cluster\nsil_widths <- summary(sil[[which(10:50 == best_k)]])[[2]]\nnames(sil_widths) <- levels(clust)\n# rearrange clusters in order of silhouette widths\norder <- order(sil_widths, decreasing = F)\nclust <- factor(clust, levels(clust)[order])\n\n\n\n# compare medoid and frequency cluster names\ndata.frame(med = medoid[order], freq = levels(clust))\n\n#clust <- factor(clust, levels(clust)[order])\n\n\n\n# word frequencies\nfrequencies <- table(words)[tolower(names(clust))]\n\n# silhouette widths\nsil_words <- rownames(GoogleNews_lowercase_cos)\nsil_widths <- sil[[which.max(avg_sil)]][sil_words,3]\n\n\n\n# select 10 clusters with the largest widths\nset.seed(123)\nX <- 1 - GoogleNews_lowercase_cos\n\n# rank based on quality of cluster and size of cluster (we want the clusters which are large AND have large widths)\n#ranking <- order(order(unique(widths), decreasing = T)) #+ order(order(counts, decreasing = T))\n\n#selected_clusters <- as.character(unique(clust)[which(ranking %in% sort(ranking, decreasing = F)[1:10])])\n\n# randomly select 10 word clusters\n\nselected_clusters <- unique(clust)[sort(sample(length(unique(clust)), 10))]\nclust_sample <- clust[clust %in% selected_clusters]\nX_sample <- X[names(clust_sample), names(clust_sample)]\nfrequencies_sample <- frequencies[tolower(names(clust_sample))]\nwidths_sample <- sil_widths[rownames(X_sample)]\n\n\nX_sample_avg <- X_sample\nfor(i in unique(clust_sample)) {\n  for(j in unique(clust_sample)) {\n  words_clust_row <- clust[clust == i]\n  words_clust_col <- clust[clust == j]\n  X_sample_avg[names(words_clust_row), names(words_clust_col)] <- median(unlist((X_sample[names(words_clust_row), names(words_clust_col)])))\n  }\n}\n\npng(\"Figures/GoogleNews.png\", width = 1000, height = 700)\nsuperheat(X = X_sample_avg, # plot SIMILARITY matrix\n          membership.rows = clust_sample,\n          membership.cols = clust_sample,\n          heat.pal = brewer.pal(10, \"RdBu\")[-c(1:2, 5)],\n          #heat.pal.values = c(0, 0.1, 1),\n          bottom.text.angle = 90,\n          left.text.angle = 0,\n          left.label.size = 0.2,\n          box.size = 1.5,\n          order.rows = order(widths_sample),\n          order.cols = order(widths_sample),\n          \n          yr = widths_sample,\n          yr.plot.type = \"bar\",\n          yr.bar.col = \"black\",\n          yr.axis.name = \"Silhouette width\",\n          yr.plot.size = 0.4,\n          yr.axis.size = 15,\n          yr.axis.name.size = 15,\n          \n          left.text.size = 7,\n          \n          bottom.text.size = 7,\n          bottom.label.size = 0.25,\n          \n          legend.size = 4)\ndev.off()\n\n\n\n###### make a word cloud for each cluster ####\nlibrary(wordcloud)\ncluster <- \"deal\"\n\n\npar(mfrow=c(1,4))\nset.seed(1234)\npng(\"Figures/wordcloud_four.png\", width = 1000, height = 1000)\nfor (cluster in sample(unique(clust_sample), length(unique(clust_sample)))[1:4]) {\n  #png(paste0(\"Figures/wordcloud_\",cluster,\".png\"), width = 500, height = 500)\n  cluster_words <- names(clust_sample[clust_sample == cluster])\n  cols <- rep(\"black\", length(cluster_words))\n  cols[which(cluster_words == cluster)] <- \"firebrick3\"\n  a <- wordcloud(names(clust_sample[clust_sample == cluster]), \n          frequencies[names(clust_sample[clust_sample == cluster])],\n          colors = cols,\n          ordered.colors = T,\n          random.order = F)\n  #dev.off()\n}\ndev.off()",
    "created" : 1474997921492.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2955151915",
    "id" : "761877DA",
    "lastKnownWriteTime" : 1456861063,
    "last_content_update" : 1456861063,
    "path" : "~/Google Drive/Berkeley PhD/Posters/16-03 BSTARS (superheat)/GoogleNews_superheat.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}